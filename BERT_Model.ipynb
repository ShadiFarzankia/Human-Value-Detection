{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT\n",
        "\n",
        "### Introduction\n",
        "In this notebook, our objective is to train an BERT model for a challenging multilabel classification task. This task involves categorizing textual arguments into one or more of 20 distinct categories, each representing a fundamental human value.\n",
        "\n",
        "The categories are as follows:\n",
        "- Self-direction: thought\n",
        "- Self-direction: action\n",
        "- Stimulation\n",
        "- Hedonism\n",
        "- Achievement\n",
        "- Power: dominance\n",
        "- Power: resources\n",
        "- Face\n",
        "- Security: personal\n",
        "- Security: societal\n",
        "- Tradition\n",
        "- Conformity: rules\n",
        "- Conformity: interpersonal\n",
        "- Humility\n",
        "- Benevolence: caring\n",
        "- Benevolence: dependability\n",
        "- Universalism: concern\n",
        "- Universalism: nature\n",
        "- Universalism: tolerance\n",
        "- Universalism: objectivity\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yoQ0lKhx7JnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Flow of the notebook\n",
        "\n",
        "\n",
        "The notebook will be structured into distinct sections to offer a well-organized guide through the implemented process. These sections will include:\n",
        "\n",
        "1. Installing the required libraries\n",
        "2. Importing the libraries\n",
        "3. Loading the datasets\n",
        "3. Defining and Fine-Tuning the Model\n",
        "  - BERT Base\n",
        "  - BERT Large\n",
        "4. Evaluation"
      ],
      "metadata": {
        "id": "XWQbJ1P_7TCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing the Required Libraries\n",
        "\n",
        "We have to install the libraries below because it is not be pre-installed in the runtime environment provided by Google Colab:\n",
        "- transformers\n",
        "- SentencePiece\n",
        "- wandb\n",
        "- simpletransformers"
      ],
      "metadata": {
        "id": "aEnEdPCo74Zj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gNHkPmU70ez"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SentencePiece"
      ],
      "metadata": {
        "id": "nGOgwwjd8Hrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb --upgrade"
      ],
      "metadata": {
        "id": "NpKM4_MPIMFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade wandb simpletransformers"
      ],
      "metadata": {
        "id": "G8H9M2oTcKyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simpletransformers"
      ],
      "metadata": {
        "id": "_edMZrO28JX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the Libraries"
      ],
      "metadata": {
        "id": "o2ncKBbL8MMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "import logging\n",
        "import wandb\n",
        "\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from simpletransformers.classification import MultiLabelClassificationModel, ClassificationArgs\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, classification_report"
      ],
      "metadata": {
        "id": "7Fr2hUjM8P-g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Data\n",
        "We utilize data sourced from [Zenodo](https://zenodo.org/record/7550385#.Y8wMquzMK3I), specifically from the [Human Value Detection 2023 competition](https://touche.webis.de/semeval23/touche23-web/index.html). Our focus is on the following datasets: arguments-training.tsv, arguments-validation.tsv, arguments-test.tsv, labels-training.tsv, labels-validation.tsv, and labels-test.tsv.\n"
      ],
      "metadata": {
        "id": "7KL5rVH18Rcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the label titles in our datasets\n",
        "label_cols = ['Self-direction: thought', 'Self-direction: action', 'Stimulation',\n",
        "       'Hedonism', 'Achievement', 'Power: dominance', 'Power: resources',\n",
        "       'Face', 'Security: personal', 'Security: societal', 'Tradition',\n",
        "       'Conformity: rules', 'Conformity: interpersonal', 'Humility',\n",
        "       'Benevolence: caring', 'Benevolence: dependability',\n",
        "       'Universalism: concern', 'Universalism: nature',\n",
        "       'Universalism: tolerance', 'Universalism: objectivity']"
      ],
      "metadata": {
        "id": "Sf1yw97Xyo6P"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data\n",
        "train_args = pd.read_csv(\"arguments-training.tsv\",delimiter='\\t')\n",
        "train_labels = pd.read_csv(\"labels-training.tsv\",delimiter='\\t')\n",
        "\n",
        "val_labels = pd.read_csv(\"labels-validation.tsv\",delimiter='\\t')\n",
        "val_args = pd.read_csv(\"arguments-validation.tsv\",delimiter='\\t')\n",
        "\n",
        "test_labels = pd.read_csv(\"labels-test.tsv\",delimiter='\\t')\n",
        "test_args = pd.read_csv(\"arguments-test.tsv\",delimiter='\\t')"
      ],
      "metadata": {
        "id": "WnKxwEPs8W-S"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input data for simple transformers' models need to have a 'text' column containing the context we need to apply multiclassification and the list of labels for each element in the context."
      ],
      "metadata": {
        "id": "6MND0ySO8ION"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the 'text' column\n",
        "train_args['text'] = train_args['Conclusion'] + \" \" + train_args['Stance'] + \" \" + train_args['Premise']\n",
        "train_args.drop(labels=['Conclusion', 'Stance', 'Premise'], axis=1, inplace=True)\n",
        "\n",
        "val_args['text'] = val_args['Conclusion'] + \" \" + val_args['Stance'] + \" \" + val_args['Premise']\n",
        "val_args.drop(labels=['Conclusion', 'Stance', 'Premise'], axis=1, inplace=True)\n",
        "\n",
        "test_args['text'] = test_args['Conclusion'] + \" \" + test_args['Stance'] + \" \" + test_args['Premise']\n",
        "test_args.drop(labels=['Conclusion', 'Stance', 'Premise'], axis=1, inplace=True)\n",
        "\n",
        "print(val_labels)"
      ],
      "metadata": {
        "id": "FI-dEUz58Y4R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0632b2e0-102b-45ac-c2d7-70a3327e26b0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Argument ID  Self-direction: thought  Self-direction: action  \\\n",
            "0         A01001                        0                       0   \n",
            "1         A01012                        0                       0   \n",
            "2         A02001                        0                       0   \n",
            "3         A02002                        0                       1   \n",
            "4         A02009                        0                       0   \n",
            "...          ...                      ...                     ...   \n",
            "1891      E08014                        1                       0   \n",
            "1892      E08021                        1                       0   \n",
            "1893      E08022                        0                       1   \n",
            "1894      E08024                        0                       1   \n",
            "1895      E08025                        0                       1   \n",
            "\n",
            "      Stimulation  Hedonism  Achievement  Power: dominance  Power: resources  \\\n",
            "0               0         0            0                 0                 0   \n",
            "1               0         0            0                 0                 0   \n",
            "2               0         0            0                 0                 0   \n",
            "3               0         0            0                 0                 0   \n",
            "4               0         0            0                 0                 0   \n",
            "...           ...       ...          ...               ...               ...   \n",
            "1891            0         0            1                 0                 0   \n",
            "1892            0         0            0                 0                 0   \n",
            "1893            0         0            0                 0                 0   \n",
            "1894            0         0            0                 1                 0   \n",
            "1895            0         0            0                 0                 0   \n",
            "\n",
            "      Face  Security: personal  ...  Tradition  Conformity: rules  \\\n",
            "0        0                   0  ...          0                  0   \n",
            "1        0                   0  ...          0                  0   \n",
            "2        0                   1  ...          0                  0   \n",
            "3        0                   0  ...          0                  0   \n",
            "4        0                   0  ...          0                  1   \n",
            "...    ...                 ...  ...        ...                ...   \n",
            "1891     0                   1  ...          0                  1   \n",
            "1892     0                   0  ...          0                  1   \n",
            "1893     0                   0  ...          0                  1   \n",
            "1894     0                   1  ...          0                  0   \n",
            "1895     0                   0  ...          0                  0   \n",
            "\n",
            "      Conformity: interpersonal  Humility  Benevolence: caring  \\\n",
            "0                             0         0                    0   \n",
            "1                             0         0                    0   \n",
            "2                             0         0                    0   \n",
            "3                             0         0                    0   \n",
            "4                             0         0                    0   \n",
            "...                         ...       ...                  ...   \n",
            "1891                          0         0                    0   \n",
            "1892                          0         0                    0   \n",
            "1893                          0         0                    0   \n",
            "1894                          0         0                    0   \n",
            "1895                          0         0                    0   \n",
            "\n",
            "      Benevolence: dependability  Universalism: concern  Universalism: nature  \\\n",
            "0                              0                      0                     0   \n",
            "1                              0                      1                     0   \n",
            "2                              0                      1                     0   \n",
            "3                              0                      0                     0   \n",
            "4                              0                      1                     0   \n",
            "...                          ...                    ...                   ...   \n",
            "1891                           0                      1                     0   \n",
            "1892                           1                      1                     0   \n",
            "1893                           1                      1                     0   \n",
            "1894                           0                      1                     0   \n",
            "1895                           0                      1                     0   \n",
            "\n",
            "      Universalism: tolerance  Universalism: objectivity  \n",
            "0                           0                          0  \n",
            "1                           0                          0  \n",
            "2                           0                          0  \n",
            "3                           0                          0  \n",
            "4                           0                          1  \n",
            "...                       ...                        ...  \n",
            "1891                        0                          1  \n",
            "1892                        0                          1  \n",
            "1893                        0                          1  \n",
            "1894                        0                          1  \n",
            "1895                        0                          0  \n",
            "\n",
            "[1896 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_labels(data):\n",
        "  df = pd.DataFrame(data)\n",
        "\n",
        "  final_list = df.copy()  # Make a copy of the DataFrame\n",
        "  final_list['labels'] = df.iloc[:, 1:].apply(lambda row: tuple(row), axis=1)\n",
        "\n",
        "  return final_list"
      ],
      "metadata": {
        "id": "SBUXQjiE8c1a"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the 'labels' column\n",
        "train_labels_compressed = prepare_labels(train_labels)\n",
        "val_labels_compressed = prepare_labels(val_labels)\n",
        "test_labels_compressed = prepare_labels(test_labels)"
      ],
      "metadata": {
        "id": "eL2AyF638kKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the content of two files into one dataset\n",
        "merged_train = pd.merge(train_args, train_labels_compressed, on='Argument ID', how='inner')\n",
        "merged_val = pd.merge(val_args, val_labels_compressed, on='Argument ID', how='inner')\n",
        "merged_test = pd.merge(test_args, test_labels_compressed, on='Argument ID', how='inner')"
      ],
      "metadata": {
        "id": "7Ww6CXCI8sxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have implemented the following functions to determine the optimal threshold after training and assessing the model. This is necessary because the model's output consists of a series of probabilities, and these probabilities need to be transformed into binary values (0s and 1s) using a threshold."
      ],
      "metadata": {
        "id": "Em5c0ibs92X3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtaining the best optimal threshold\n",
        "def get_threshold(labels, model_output):\n",
        "  results = {}\n",
        "  labels_compressed = labels.drop(\"Argument ID\", axis=1)\n",
        "\n",
        "  df_prediction = labels_compressed.copy()\n",
        "  for tr in np.arange(0.1, 0.9, 0.05):\n",
        "      tr = round(tr, 2)\n",
        "      for i, label in enumerate(label_cols):\n",
        "          prediction = np.where(model_output[:,i] >= tr, 1, 0)\n",
        "          df_prediction[label] = prediction\n",
        "\n",
        "      y_pred = df_prediction.values.tolist()\n",
        "      y_test = labels_compressed.values.tolist()\n",
        "      f1 = f1_score(y_test, y_pred, average = \"macro\", zero_division = 1)\n",
        "      results[tr] = f1\n",
        "\n",
        "  for k,v in results.items():\n",
        "      print(\"THRESHOLD: {:.2f} \".format(k), \"F1 score: {:.3f}\".format(v))\n",
        "\n",
        "  THRESHOLD = max(results, key = results.get)\n",
        "\n",
        "  print(\"\\nBest threshold obtained:\", THRESHOLD , \"having F1 score of: {:.2f}\".format(max(results.values())))\n",
        "\n",
        "  return THRESHOLD"
      ],
      "metadata": {
        "id": "M7uCt-tL435f"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the function below, we can get a report of the classification scores on various classes."
      ],
      "metadata": {
        "id": "-zz7Q6qV_ct3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtaining the final report of our scores\n",
        "def get_report(labels, model_output, THRESHOLD):\n",
        "  print(labels)\n",
        "  print(model_output)\n",
        "  print(THRESHOLD)\n",
        "  df_prediction = labels.copy()\n",
        "\n",
        "  for i, label in enumerate(label_cols):\n",
        "    prediction = np.where(model_output[:,i] >= THRESHOLD, 1, 0)\n",
        "    df_prediction[label] = prediction\n",
        "\n",
        "  y_pred = df_prediction.values.tolist()\n",
        "  y_test = labels.values.tolist()\n",
        "\n",
        "  print(classification_report(y_test,y_pred, target_names = label_cols))"
      ],
      "metadata": {
        "id": "1TWN0K0w5da_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Model\n",
        "\n",
        "\n",
        "In this section, we introduce our BERT model and initiate the fine-tuning process by exploring various parameters. To facilitate this experimentation, we make use of the Sweep library, which enables us to systematically evaluate different model configurations.\n",
        "\n",
        "We work with two versions of the BERT model, namely \"bert-base-cased\" and \"bert-large-cased.\" However, owing to constraints with our CUDA resources, we begin by fine-tuning \"bert-base-cased\" using hyperparameter tuning to identify the optimal hyperparameters. Once we've determined the best hyperparameters for \"bert-base-cased,\" we proceed to train both the \"bert-base-cased\" and \"bert-large-cased\" models. This approach allows us to make a fair and meaningful comparison of their performance on our datasets."
      ],
      "metadata": {
        "id": "7K5ZVKpEeFmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Hyperparameter\n",
        "\n",
        "\n",
        "We attempted to conduct experiments by varying the parameters related to the number of training epochs and the batch size."
      ],
      "metadata": {
        "id": "-DcNJM_9KSjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparmaters we want to fine-tune\n",
        "sweep_config = {\n",
        "    \"method\": \"grid\",  # grid, random\n",
        "    \"parameters\": {\n",
        "        \"num_train_epochs\": {\"values\": [3, 5, 10]},\n",
        "        \"train_batch_size\": {\"values\": [16, 32]}\n",
        "    },\n",
        "    'metric': {\n",
        "        'name': 'eval_loss',  # or 'LRAP', 'accuracy', etc.\n",
        "        'goal': 'minimize'    # or 'maximize'\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "OGLLHlc3W_B5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a sweep in W&B\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"Human Value Detection\")"
      ],
      "metadata": {
        "id": "pqwv4SWLXTTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)"
      ],
      "metadata": {
        "id": "rV5T6eF4XTWG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our fixed arguments for the simple transformers model\n",
        "args = {\n",
        "      'manual_seed': 42,\n",
        "      'max_seq_length': 100,\n",
        "      'overwrite_output_dir': True,\n",
        "      'gradient_accumulation_steps': 8,\n",
        "      \"lr\": 2e-4,\n",
        "      \"optimizer\": 'AdamW'\n",
        "      }"
      ],
      "metadata": {
        "id": "ldCIZBusVX9T"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT Base\n",
        "This section contains the training phase for our BERT base model:"
      ],
      "metadata": {
        "id": "LvLbxUdaNVXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init()\n",
        "\n",
        "    # Create a TransformerModel\n",
        "    model = MultiLabelClassificationModel(\n",
        "        'bert',\n",
        "        'bert-base-cased',\n",
        "        use_cuda=True,\n",
        "        args=args,\n",
        "        sweep_config=wandb.config,\n",
        "        num_labels=20\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model.train_model(merged_train)\n",
        "\n",
        "    # Evaluate the model\n",
        "    model.eval_model(merged_val)\n",
        "\n",
        "    # Sync wandb\n",
        "    wandb.join()\n"
      ],
      "metadata": {
        "id": "XxUzxpBRXcRW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, train, count=6)"
      ],
      "metadata": {
        "id": "9WRPg6JrXfMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the W&B API\n",
        "api = wandb.Api()\n",
        "\n",
        "# Replace 'your_project_name' with your actual project name\n",
        "project = api.project('Human Value Detection')\n",
        "\n",
        "project"
      ],
      "metadata": {
        "id": "7PxRpBThnWw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the sweep\n",
        "sweep = api.sweep(f\"{project.name}/{sweep_id}\")\n",
        "\n",
        "# Retrieve the best run\n",
        "best_run = sweep.best_run()\n",
        "\n",
        "# Get the best run's parameters and results\n",
        "best_params = best_run.config\n",
        "best_results = best_run.summary"
      ],
      "metadata": {
        "id": "ut206MXvo2AJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c48f2ce-3648-4f3e-b9d2-14124a156dfc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sorting runs by +summary_metrics.eval_loss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Results:\", best_results)"
      ],
      "metadata": {
        "id": "8IsW7k9EpqbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e595ee54-1fb3-47bd-85cf-d8fd5f6a1d0f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'num_train_epochs': 3, 'train_batch_size': 32}\n",
            "Best Results: {'Training loss': 0.41010457277297974, '_runtime': 132.145995657, '_step': 0, '_timestamp': 1746645539.280375, '_wandb': {'runtime': 202}, 'global_step': 50, 'lr': 8.8135593220339e-06}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Defining the model using the best parameters obtained\n",
        " model_bert_base = MultiLabelClassificationModel(\n",
        "        'bert',\n",
        "        'bert-base-cased',\n",
        "        use_cuda=True,\n",
        "        args={\n",
        "              'num_train_epochs': best_params['num_train_epochs'],\n",
        "              'train_batch_size': best_params['train_batch_size'],\n",
        "              'manual_seed': 42,\n",
        "              'max_seq_length': 100,\n",
        "              'overwrite_output_dir': True,\n",
        "              'gradient_accumulation_steps': 8,\n",
        "              \"lr\": 2e-4,\n",
        "              \"optimizer\": 'AdamW'\n",
        "            },\n",
        "        num_labels=20\n",
        ")"
      ],
      "metadata": {
        "id": "xQEPJQJ8pxFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "231ed213-36eb-43b9-fffb-ffe0e3592923"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForMultiLabelSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# layers of our bert base model\n",
        "model_bert_base.model"
      ],
      "metadata": {
        "id": "UtxiH7OqrQB3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9331791c-ca42-4d3c-8489-30da2af49bb7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMultiLabelSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model\n",
        "model_bert_base.train_model(merged_train, eval_df=merged_val)"
      ],
      "metadata": {
        "id": "8X9lmg8urSsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bert Large\n",
        "\n",
        "This section encompasses the training phase for our BERT large model."
      ],
      "metadata": {
        "id": "0wbn5Y2PBHMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining our bert large model with the hyperparameters we got from bert base\n",
        "model_bert_large = MultiLabelClassificationModel(\n",
        "        'bert',\n",
        "        'bert-large-cased',\n",
        "        use_cuda=True,\n",
        "        args={\n",
        "              'num_train_epochs': best_params['num_train_epochs'],\n",
        "              'train_batch_size': best_params['train_batch_size'],\n",
        "              'manual_seed': 42,\n",
        "              'max_seq_length': 100,\n",
        "              'overwrite_output_dir': True,\n",
        "              'gradient_accumulation_steps': 8,\n",
        "              \"lr\": 2e-4,\n",
        "              \"optimizer\": 'AdamW'\n",
        "            },\n",
        "        num_labels=20\n",
        ")"
      ],
      "metadata": {
        "id": "5Ybq_d-fBJqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Layers of our bert large model\n",
        "model_bert_large.model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6di-AfIC1Aa",
        "outputId": "f0c724ab-9f78-41f3-9377-6e7f412d3c5e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMultiLabelSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 1024, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 1024)\n",
              "      (token_type_embeddings): Embedding(2, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-23): 24 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=1024, out_features=20, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training our bert large model\n",
        "model_bert_large.train_model(merged_train, eval_df=merged_val)"
      ],
      "metadata": {
        "id": "x2bYwREZDGbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of the models\n",
        "\n",
        "\n",
        "In this section, we assess the performance of the BERT base and BERT large models on both the validation and test sets. We gauge their performance using the F1 score as the primary metric for evaluation."
      ],
      "metadata": {
        "id": "_0Oe2OLe6hR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of bert-base-cased\n",
        "In here, we evaluate the BERT base model on both the validation and test datasets."
      ],
      "metadata": {
        "id": "i28OwB0ZmfLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation on validation set**"
      ],
      "metadata": {
        "id": "-IHdame-BS2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_bert_base, model_outputs_bert_base, wrong_predictions_bert_base = model_bert_base.eval_model(merged_val)\n",
        "print(\"Results of bert base on validtion set:\\n\", result_bert_base)"
      ],
      "metadata": {
        "id": "AgwzIKW3uZVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_base_threshold = get_threshold(val_labels, model_outputs_bert_base)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S64ls6P6jJY",
        "outputId": "17166457-5f5a-4ebf-9821-1f8f8d099201"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THRESHOLD: 0.10  F1 score: 0.271\n",
            "THRESHOLD: 0.15  F1 score: 0.245\n",
            "THRESHOLD: 0.20  F1 score: 0.200\n",
            "THRESHOLD: 0.25  F1 score: 0.171\n",
            "THRESHOLD: 0.30  F1 score: 0.118\n",
            "THRESHOLD: 0.35  F1 score: 0.089\n",
            "THRESHOLD: 0.40  F1 score: 0.047\n",
            "THRESHOLD: 0.45  F1 score: 0.001\n",
            "THRESHOLD: 0.50  F1 score: 0.000\n",
            "THRESHOLD: 0.55  F1 score: 0.000\n",
            "THRESHOLD: 0.60  F1 score: 0.000\n",
            "THRESHOLD: 0.65  F1 score: 0.000\n",
            "THRESHOLD: 0.70  F1 score: 0.000\n",
            "THRESHOLD: 0.75  F1 score: 0.000\n",
            "THRESHOLD: 0.80  F1 score: 0.000\n",
            "THRESHOLD: 0.85  F1 score: 0.000\n",
            "\n",
            "Best threshold obtained: 0.1 having F1 score of: 0.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation on test set**\n"
      ],
      "metadata": {
        "id": "14gOgk8_URoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_bert_base_test, model_outputs_bert_base_test, wrong_predictions_bert_base_test = model_bert_base.eval_model(merged_test)\n",
        "print(\"Results of bert base on validtion set:\\n\", result_bert_base_test)"
      ],
      "metadata": {
        "id": "f1YPpgBtVVQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_base_threshold_test = get_threshold(test_labels, model_outputs_bert_base_test)"
      ],
      "metadata": {
        "id": "oA7aq5C3VdCV",
        "outputId": "2070448a-a3f4-445e-8ef3-69bdb3c050f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THRESHOLD: 0.10  F1 score: 0.247\n",
            "THRESHOLD: 0.15  F1 score: 0.220\n",
            "THRESHOLD: 0.20  F1 score: 0.183\n",
            "THRESHOLD: 0.25  F1 score: 0.156\n",
            "THRESHOLD: 0.30  F1 score: 0.107\n",
            "THRESHOLD: 0.35  F1 score: 0.072\n",
            "THRESHOLD: 0.40  F1 score: 0.049\n",
            "THRESHOLD: 0.45  F1 score: 0.002\n",
            "THRESHOLD: 0.50  F1 score: 0.000\n",
            "THRESHOLD: 0.55  F1 score: 0.000\n",
            "THRESHOLD: 0.60  F1 score: 0.000\n",
            "THRESHOLD: 0.65  F1 score: 0.000\n",
            "THRESHOLD: 0.70  F1 score: 0.000\n",
            "THRESHOLD: 0.75  F1 score: 0.000\n",
            "THRESHOLD: 0.80  F1 score: 0.000\n",
            "THRESHOLD: 0.85  F1 score: 0.000\n",
            "\n",
            "Best threshold obtained: 0.1 having F1 score of: 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of bert-large-cased\n",
        "This section involves the evaluation of the BERT large model on both the validation and test datasets."
      ],
      "metadata": {
        "id": "tuTz6W6kWQ8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation on validation set**"
      ],
      "metadata": {
        "id": "H08yz78DWUx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_bert_large, model_outputs_bert_large, wrong_predictions_bert_large = model_bert_large.eval_model(merged_val)\n",
        "print(\"Results of bert base on validtion set:\\n\", result_bert_large)"
      ],
      "metadata": {
        "id": "lQox1dByWbfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_large_threshold = get_threshold(val_labels, model_outputs_bert_large)"
      ],
      "metadata": {
        "id": "mK-RNdI2Wdlp",
        "outputId": "c0d76bf5-fe3c-4de2-ef3a-e66e2b687449",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THRESHOLD: 0.10  F1 score: 0.271\n",
            "THRESHOLD: 0.15  F1 score: 0.201\n",
            "THRESHOLD: 0.20  F1 score: 0.185\n",
            "THRESHOLD: 0.25  F1 score: 0.140\n",
            "THRESHOLD: 0.30  F1 score: 0.096\n",
            "THRESHOLD: 0.35  F1 score: 0.074\n",
            "THRESHOLD: 0.40  F1 score: 0.014\n",
            "THRESHOLD: 0.45  F1 score: 0.000\n",
            "THRESHOLD: 0.50  F1 score: 0.000\n",
            "THRESHOLD: 0.55  F1 score: 0.000\n",
            "THRESHOLD: 0.60  F1 score: 0.000\n",
            "THRESHOLD: 0.65  F1 score: 0.000\n",
            "THRESHOLD: 0.70  F1 score: 0.000\n",
            "THRESHOLD: 0.75  F1 score: 0.000\n",
            "THRESHOLD: 0.80  F1 score: 0.000\n",
            "THRESHOLD: 0.85  F1 score: 0.000\n",
            "\n",
            "Best threshold obtained: 0.1 having F1 score of: 0.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation on test set**"
      ],
      "metadata": {
        "id": "Az-KIUUsYAUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_bert_large_test, model_outputs_bert_large_test, wrong_predictions_bert_large_test = model_bert_large.eval_model(merged_test)\n",
        "print(\"Results of bert base on validtion set:\\n\", result_bert_large_test)"
      ],
      "metadata": {
        "id": "JG58iBriYQFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_large_threshold_test = get_threshold(test_labels, model_outputs_bert_large_test)"
      ],
      "metadata": {
        "id": "daotfTNSYZ8Z",
        "outputId": "a936c218-c16c-417c-8d70-8cfe73e51f42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THRESHOLD: 0.10  F1 score: 0.244\n",
            "THRESHOLD: 0.15  F1 score: 0.186\n",
            "THRESHOLD: 0.20  F1 score: 0.155\n",
            "THRESHOLD: 0.25  F1 score: 0.139\n",
            "THRESHOLD: 0.30  F1 score: 0.102\n",
            "THRESHOLD: 0.35  F1 score: 0.070\n",
            "THRESHOLD: 0.40  F1 score: 0.017\n",
            "THRESHOLD: 0.45  F1 score: 0.000\n",
            "THRESHOLD: 0.50  F1 score: 0.000\n",
            "THRESHOLD: 0.55  F1 score: 0.000\n",
            "THRESHOLD: 0.60  F1 score: 0.000\n",
            "THRESHOLD: 0.65  F1 score: 0.000\n",
            "THRESHOLD: 0.70  F1 score: 0.000\n",
            "THRESHOLD: 0.75  F1 score: 0.000\n",
            "THRESHOLD: 0.80  F1 score: 0.000\n",
            "THRESHOLD: 0.85  F1 score: 0.000\n",
            "\n",
            "Best threshold obtained: 0.1 having F1 score of: 0.24\n"
          ]
        }
      ]
    }
  ]
}